{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 基础邮件分类器\n",
        "\n",
        "实现了一个基础的邮件分类器，使用朴素贝叶斯算法对垃圾邮件进行分类。\n",
        "- 文本预处理和分词\n",
        "- 高频词特征提取\n",
        "- 朴素贝叶斯分类\n",
        "- 邮件分类预测\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\CMH\\.conda\\envs\\NLP\\Lib\\site-packages\\jieba\\_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "c:\\Users\\CMH\\.conda\\envs\\NLP\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:11: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
            "  from scipy.sparse import csr_matrix, issparse\n"
          ]
        }
      ],
      "source": [
        "## 导入必要的库\n",
        "import re\n",
        "import os\n",
        "from jieba import cut # type: ignore\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB # type: ignore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 文本预处理函数\n",
        "def get_words(filename):\n",
        "    \"\"\"读取文本并过滤无效字符和长度为1的词\"\"\"\n",
        "    words = []\n",
        "    with open(filename, 'r', encoding='utf-8') as fr:\n",
        "        for line in fr:\n",
        "            line = line.strip()\n",
        "            # 过滤无效字符\n",
        "            line = re.sub(r'[.【】0-9、——。，！~\\*]', '', line)\n",
        "            # 使用jieba.cut()方法对文本切词处理\n",
        "            line = cut(line)\n",
        "            # 过滤长度为1的词\n",
        "            line = filter(lambda word: len(word) > 1, line)\n",
        "            words.extend(line)\n",
        "    return words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache C:\\Users\\CMH\\AppData\\Local\\Temp\\jieba.cache\n",
            "Loading model cost 1.130 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "提取了 100 个高频词作为特征\n",
            "前10个高频词: ['华为', '我们', '企业', '人工智能', '智能', '技术', '中国', '实现', '发展', '可以']\n"
          ]
        }
      ],
      "source": [
        "## 构建词库和特征提取\n",
        "all_words = []\n",
        "\n",
        "def get_top_words(top_num):\n",
        "    \"\"\"遍历邮件建立词库后返回出现次数最多的词\"\"\"\n",
        "    filename_list = ['邮件_files/{}.txt'.format(i) for i in range(151)]\n",
        "    # 遍历邮件建立词库\n",
        "    for filename in filename_list:\n",
        "        all_words.append(get_words(filename))\n",
        "    # itertools.chain()把all_words内的所有列表组合成一个列表\n",
        "    # collections.Counter()统计词个数\n",
        "    freq = Counter(chain(*all_words))\n",
        "    return [i[0] for i in freq.most_common(top_num)]\n",
        "\n",
        "# 获取最常见的100个词\n",
        "top_words = get_top_words(100)\n",
        "print(f\"提取了 {len(top_words)} 个高频词作为特征\")\n",
        "print(\"前10个高频词:\", top_words[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "特征矩阵形状: (151, 100)\n",
            "标签分布: 垃圾邮件 127 个, 普通邮件 24 个\n",
            "模型训练完成!\n"
          ]
        }
      ],
      "source": [
        "## 构建特征向量和训练模型\n",
        "# 构建词-个数映射表\n",
        "vector = []\n",
        "for words in all_words:\n",
        "    # 统计每个高频词在当前文档中出现的次数\n",
        "    word_map = list(map(lambda word: words.count(word), top_words))\n",
        "    vector.append(word_map)\n",
        "\n",
        "vector = np.array(vector)\n",
        "print(f\"特征矩阵形状: {vector.shape}\")\n",
        "\n",
        "# 0-126.txt为垃圾邮件标记为1；127-151.txt为普通邮件标记为0\n",
        "labels = np.array([1]*127 + [0]*24)\n",
        "print(f\"标签分布: 垃圾邮件 {sum(labels)} 个, 普通邮件 {len(labels) - sum(labels)} 个\")\n",
        "\n",
        "# 训练朴素贝叶斯模型\n",
        "model = MultinomialNB()\n",
        "model.fit(vector, labels)\n",
        "print(\"模型训练完成!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "测试邮件分类结果:\n",
            "151.txt分类情况: 垃圾邮件\n",
            "152.txt分类情况: 垃圾邮件\n",
            "153.txt分类情况: 垃圾邮件\n",
            "154.txt分类情况: 垃圾邮件\n",
            "155.txt分类情况: 普通邮件\n"
          ]
        }
      ],
      "source": [
        "## 定义预测函数并测试\n",
        "def predict(filename):\n",
        "    \"\"\"对未知邮件分类\"\"\"\n",
        "    # 构建未知邮件的词向量\n",
        "    words = get_words(filename)\n",
        "    current_vector = np.array(\n",
        "        tuple(map(lambda word: words.count(word), top_words)))\n",
        "    # 预测结果\n",
        "    result = model.predict(current_vector.reshape(1, -1))\n",
        "    return '垃圾邮件' if result == 1 else '普通邮件'\n",
        "\n",
        "# 对测试邮件进行分类\n",
        "test_files = ['151.txt', '152.txt', '153.txt', '154.txt', '155.txt']\n",
        "print(\"测试邮件分类结果:\")\n",
        "for file in test_files:\n",
        "    try:\n",
        "        result = predict(f'邮件_files/{file}')\n",
        "        print(f'{file}分类情况: {result}')\n",
        "    except FileNotFoundError:\n",
        "        print(f'{file}: 文件不存在')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
